[id='enabling-data-science-pipelines']
= Enabling data science pipelines

NOTE: If you do not intend to complete the pipelines section of the workshop you can skip this step and move on to the next section, xref:creating-a-workbench.adoc[Create a Workbench].

In this section, you prepare your {deliverable} environment so that you can use data science pipelines.

In this {deliverable}, you implement an example pipeline by using the JupyterLab Elyra extension. With Elyra, you can create a visual end-to-end pipeline workflow that can be executed in OpenShift AI.

.Prerequisite

* You have installed local object storage buckets and created data connections, as described in xref:storing-data-with-data-connections.adoc[Storing data with data connections].

.Procedure

. In the {productname-short} dashboard, click *Data Science Projects* and then select *Fraud Detection*.

. Click the *Pipelines* tab.

. Click *Configure pipeline server*.
+
image::projects/ds-project-create-pipeline-server.png[Create pipeline server button]

. In the *Configure pipeline server* form, in the *Access key* field next to the key icon, click the dropdown menu and then click *Pipeline Artifacts* to populate the *Configure pipeline server* form with credentials for the data connection.
+
image::projects/ds-project-create-pipeline-server-form.png[Selecting the Pipeline Artifacts data connection]

. Leave the database configuration as the default.

. Click *Configure pipeline server*.

. Wait until the spinner disappears and *Start by importing a pipeline* is displayed.
+
[IMPORTANT]
====
You must wait until the pipeline configuration is complete before you continue and create your workbench. If you create your workbench before the pipeline server is ready, your workbench will not be able to submit pipelines to it.
====
+
If you have waited more than 5 minutes, and the pipeline server configuration does not complete, you can try to delete the pipeline server and create it again.
+
image::projects//ds-project-delete-pipeline-server.png[Delete pipeline server, 250]
+
You can also ask your {productname-short} administrator to verify that self-signed certificates are added to your cluster as described in link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/installing_and_uninstalling_openshift_ai_self-managed/working-with-certificates_certs[Working with certificates].

.Verification

. Navigate to the *Pipelines* tab for the project.
. Next to *Import pipeline*, click the action menu (&#8942;) and then select *View pipeline server configuration*.
+
image::projects/ds-project-pipeline-server-view.png[View pipeline server configuration menu, 250]
+
An information box opens and displays the object storage connection information for the pipeline server.

.Next step

xref:creating-a-workbench.adoc[Creating a workbench and selecting a notebook image]

//xref:automating-workflows-with-pipelines.adoc[Automating workflows with data science pipelines]

//xref:running-a-pipeline-generated-from-python-code.adoc[Running a data science pipeline generated from Python code]
